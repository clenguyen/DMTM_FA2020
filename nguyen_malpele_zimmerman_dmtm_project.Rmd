---
title: "Data Mining and Text Mining - Final Project"
author: "Cindy Nguyen, Luiz Gustavo Fagundes Malpele, Isabel Zimmerman"
output:
  html_document:
    toc: true
    toc_float: true
---
# Pre-processing (IZ)

```{r, message = FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(textdata)
library(igraph)
library(ggraph)
library(datetime)
```

```{r}
# reading a CSV file
corona_tweets_raw <- read_csv("./Corona_tweets.csv")
```

```{r}
corona_tweets_raw %>%
  head(size = 10)
```

Select only columns needed.
```{r}
corona_tweets <- corona_tweets_raw %>% 
  select(Time, text)
```

Create a dataframe with a column for tweet manipulation. Need to remove urls, punctuation, tagged people, and escape characters.
```{r}
corona_tweets_clean <- corona_tweets
```

Function to clean tweets
```{r}
#clean tweets
clean_tweets <- function(x) {
  x %>% 
    str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>% #remove URLs
    str_replace_all("&amp;", "and") %>% #change &amp; to and
    str_remove_all("@[[:graph:]]+") %>% #remove @
    str_remove_all("[[:punct:]]") %>% #remove punctuation
    str_replace_all("\\\n",  " ") %>% #remove \n
    str_replace_all("\\\r", " ") %>% #remove \r
    str_replace_all("\\\u0091", "") %>% #remove \u0091
    str_replace_all("\\\u0092", "") %>% #remove \u0092
    str_replace_all("\\\u0093", "") %>% #remove \u0093
    str_to_lower() #to lowercase
}
```

```{r}
corona_tweets_clean$clean_text <- clean_tweets(corona_tweets$text)
```

```{r}
corona_tweets_clean
```

```{r}
#preprocessing
 corona_tweets_clean %>% 
  unnest_tokens(word, clean_text) %>% # tokenization
  anti_join(stop_words) %>%  #remove stop words
  count(Time, word, sort = TRUE) #count words

```

# TF-IDF (Cindy)

```{r}
clean_tweet_tf_idf <- corona_tweets_clean %>% 
  unnest_tokens(word, clean_text) %>% # tokenization
  anti_join(stop_words) %>%  #remove stop words
  count(Time, word, sort = TRUE) %>%  #count words
  bind_tf_idf(word, Time, n) %>% 
  #arrange(-tf_idf) %>% 
  group_by(Time) %>% 
  top_n(20) %>% 
  ungroup()

clean_tweet_tf_idf
```



```{r}
#TF-IDF
tweet_tf_idf <- corona_tweets %>% 
  unnest_tokens(word, text) %>% # tokenization
  anti_join(stop_words) %>%   #remove stop words
  count(Time, word, sort = TRUE) %>%  #count words
  bind_tf_idf(word, Time, n) %>% 
  #arrange(-tf_idf) %>% 
  group_by(Time) %>% 
  top_n(20) %>% 
  ungroup()

tweet_tf_idf
```


Using `cast_dtm()`
```{r}
dtm_text <- clean_tweet_tf_idf %>% 
  count(Time, word, sort = TRUE) %>% 
  cast_dtm(Time, word, n)
```


Latent Dirichlet Allocation
```{r}
lda_text <- LDA(dtm_text, k = 2, control = list(seed = 1234))
lda_text
```

```{r}
text_topics <- tidy(lda_text, matrix = "beta")
text_topics
```

```{r}
top_terms <- text_topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms
```

```{r}
graph_topics <- top_terms %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% 
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  labs(x = "", y = expression(beta)) +
  theme_minimal()
  

graph_topics
```


# Bigrams (Gustavo)

```{r}
bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co")) %>% 
  filter(!word2 %in% c("https", "19", "t.co")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 140)
  
bigram_filtered
```


```{r}
bigram_graph <- bigram_filtered %>% 
  graph_from_data_frame()

bigram_graph
```

Create plot:

```{r}
set.seed(6648)

# plot graph
bigrams_network <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

bigrams_network
```

#Trigrams

```{r}
trigram_filtered <- corona_tweets %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co")) %>% 
  filter(!word2 %in% c("https", "19", "t.co")) %>% 
  filter(!word3 %in% c("https", "19", "t.co")) %>% 
  count(word1, word2, word3, sort = TRUE) %>% 
  filter(n > 25)
  
trigram_filtered
```

```{r}
trigram_graph <- trigram_filtered %>% 
  graph_from_data_frame()

trigram_graph
```

```{r}
set.seed(6648)

# plot graph
trigrams_network <- ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

trigrams_network
```

#Bigram Analysis

```{r}
china_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "3", "4")) %>% 
  filter(word1 %in% c("china", "chinese", "wuhan") | word2 %in% c("china", "chinese", "wuhan")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
china_bigram_filtered
```

```{r}
china_bigram_graph <- china_bigram_filtered %>% 
  graph_from_data_frame()

china_bigram_graph
```

```{r}
# plot graph
china_bigrams_network <- ggraph(china_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

china_bigrams_network
```

#Exploring Italy

```{r}
italy_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(word1 %in% c("italy", "italian", "europe") | word2 %in% c("italy", "italian", "europe")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
italy_bigram_filtered
```

```{r}
italy_bigram_graph <- italy_bigram_filtered %>% 
  graph_from_data_frame()

italy_bigram_graph
```

```{r}
# plot graph
italy_bigrams_network <- ggraph( italy_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

italy_bigrams_network
```

```{r}
fakenews_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(word1 %in% c("conspiracy", "theory", "fake", "hoax", "created", "laboratory", "fakenews", "media") | word2 %in% c("conspiracy", "theory", "fake", "hoax", "created", "laboratory", "fakenews", "media")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
fakenews_bigram_filtered
```

```{r}
fakenews_bigram_graph <- fakenews_bigram_filtered %>% 
  graph_from_data_frame()

fakenews_bigram_graph
```

```{r}
# plot graph
fakenews_bigrams_network <- ggraph( fakenews_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = arrow(length = unit(2, "mm"))) +
  geom_node_point(color = "#53316B", 
                  size = 2) +
  geom_node_text(aes(label = name), 
                 vjust = 1.5, hjust = 0.2, size = 3) + 
  theme_void()

fakenews_bigrams_network
```