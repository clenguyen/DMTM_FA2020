---
title: "Data Mining and Text Mining - Final Project"
author: "Cindy Nguyen, Luiz Gustavo Fagundes Malpele, Isabel Zimmerman"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
---
# Introduction

This project provides text mining and analysis from COVID-19 tweets found in [this dataset](https://www.kaggle.com/datatattle/coronavirus-tweets). This analysis utilizes tf-idf, LDA, and n-grams in order to create a better understanding of the feelings towards the novel coronavirus from March 16, 2020 to April 14, 2020. Conclusions include that the sentiment is largely regionally based. Future work on this project could explore sentiment drift further into 2020. 

# Pre-processing

```{r, message = FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(textdata)
library(igraph)
library(ggraph)
library(datetime)
```

```{r, message = FALSE, echo=FALSE}
# reading CSV file
corona_tweets_raw <- read_csv("./Corona_tweets.csv")
```

```{r}
corona_tweets_raw %>%
  head(size = 10)
```

## Data
The data pulled from Kaggle dataset [here](https://www.kaggle.com/datatattle/coronavirus-tweets) offers:

- `User_name`: deidentified as a number
- `Time`: dates in the form of DD-MM-YYYY dates
- `Location`: self-identified user location from twitter bio
- `text`: the tweet itself

For this analysis, the primary analysis uses  `text` to gather a sentiment overview. This data is stripped of punctuation, other tagged users, excape characters, and links in order to ensure data quality. 

```{r, echo=FALSE}
corona_tweets <- corona_tweets_raw %>% 
  select(Time, text)
```

We used a function to clean tweets, as it creates and easily reproducible set and makes updating how tweets need to be cleaned very easy.
```{r}
#clean tweets
clean_tweets <- function(x) {
  x %>% 
    str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>% #remove URLs
    str_replace_all("&amp;", "and") %>% #change &amp; to and
    str_remove_all("@[[:graph:]]+") %>% #remove @
    str_remove_all("[[:punct:]]") %>% #remove punctuation
    str_replace_all("\\\n",  " ") %>% #remove \n
    str_replace_all("\\\r", " ") %>% #remove \r
    str_replace_all("\\\u0091", "") %>% #remove \u0091
    str_replace_all("\\\u0092", "") %>% #remove \u0092
    str_replace_all("\\\u0093", "") %>% #remove \u0093
    str_to_lower() #to lowercase
}
```

Next, we clean the raw text and put it in a new column called `clean_text` in a new dataframe, _corona_tweets_.
```{r}
corona_tweets$clean_text <- clean_tweets(corona_tweets_raw$text)

corona_tweets
```

We'll take a quick peek at the most used words overall in order to do a quick sanity check that the data makes sense.
```{r}
# view most-used words
 corona_tweets %>% 
  unnest_tokens(word, clean_text) %>% # tokenization
  anti_join(stop_words) %>%  #remove stop words
  count(word, sort = TRUE) #count words
```



# TF-IDF

```{r}
corona_tweet_tf_idf <- corona_tweets %>% 
  unnest_tokens(word, clean_text) %>% # tokenization
  anti_join(stop_words) %>%  #removing stop words
  count(Time, word, sort = TRUE) %>%  #counting words
  bind_tf_idf(word, Time, n) %>% 
  arrange(tf_idf) %>% 
  group_by(Time) %>% 
  top_n(20) %>% 
  ungroup()

corona_tweet_tf_idf
```
### Analysis

We utilized the term frequency - inverse document frequency (TF-IDF) approach to identify specific words that have been relevant between March 16th to April 14th, 2020. The news of the pandemic began to surge around the beginning of March. This caused a rift in our normal day to day activities. Restaurants, bars, schools, small businesses, and others had to close down for the next couple of months per the regulation that each state have put in place.

After preprocessing the corona tweets, stop words were removed and the TF-IDF was measured.
The output displays the trending hashtags on specific days and the counts for that hastag. The `tf_idf` column represents how relevant those words are to the dataset. For example, the word `paracetamol` trended on March 19th, 2020. This was relatively the beginning of when shutdowns began to occur. Paracetamol is also known as acetaminophen or medications like Tylenol. This medication is a fever reducer and helps treat mild to moderate pain. The take away from this is that Paracetamol was trending as this was an alternative to treat the severe symptoms of COVID-19 while a vaccine was to be made for it.  

```{r}
text_plot <- corona_tweet_tf_idf %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  filter(str_detect(Time, c("17-03-2020", "13-04-2020"))) %>% 
  ggplot(aes(x = word, y = tf_idf, fill = Time)) +
  geom_col(alpha = 0.8, show.legend = FALSE) + 
  facet_wrap(~ Time, scales = "free", ncol = 2) +
  coord_flip() +
  labs(x = "", y = "TF-IDF", title = "Highest tf-idf words in text data") +
  theme_minimal()
  

text_plot
```
### Analysis

The visualization depicts the highest tf-idf words between March 17th and April 11th, 2020. These dates were chosen to visualize the differences to when the pandemic shutdown began to about less than a month after to see what things were trending. It is apparent that COVID-19 topics would still be relevant in the two outputs. During the week of April 13th, the IRS began sending out stimulus checks to the 80 million eligible Americans through their direct deposit. This was to aid many individuals who lost their jobs or struggling to make amends meet during the early stages of the pandemic shutdown.

Some of the words here are unfamilar to the purpose of this project like `smithfield`, `quiroga`, `rmc`, `msc` and a few others. For some future work, our group could dive further into the dataset to clean the tweets even more to pertain only to COVID-related topics.


# LDA

Using `cast_dtm()`
```{r}
dtm_text <- corona_tweet_tf_idf %>% 
  count(Time, word, sort = TRUE) %>% 
  cast_dtm(Time, word, n)
```

```{r}
lda_text <- LDA(dtm_text, k = 2, control = list(seed = 1234))
lda_text
```

```{r}
text_topics <- tidy(lda_text, matrix = "beta")
text_topics
```

```{r}
top_terms <- text_topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms
```

```{r}
graph_topics <- top_terms %>% 
  mutate(topic = paste0("Topic ", topic),
         term = reorder(term, beta)) %>% 
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  coord_flip() +
  labs(x = "", y = expression(beta)) +
  theme_minimal()
  

graph_topics
```
### Analysis

Latent Dirichlet Allocation allows us to see what words are associated with each topic. Observing the two topics here, neither of them seem to have a distinction between the them. It is difficult to make a generalization about it. Topic 1 seems to cover more generic and mainstream words. Topic 2 has some relevant words that occured during the shutdown in March like `toiletpaper`, `stoppanicbuying`, and `stayhomesavelives`.



# N-grams
## Bigrams
```{r}
bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co")) %>% 
  filter(!word2 %in% c("https", "19", "t.co")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 140)
  
bigram_filtered
```


```{r}
bigram_graph <- bigram_filtered %>% 
  graph_from_data_frame()

bigram_graph
```

Create plot:

```{r}
set.seed(6648)

# plot graph
bigrams_network <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = grid::arrow(type = "closed", length = unit(.08, "inches"))) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), 
                 vjust = 1, hjust = 1, size = 3) + 
  theme_void()

bigrams_network
```

### Analysis

 When it comes to this first bigram visualization, it becomes clear that the most significant population's concern at the beginning of the pandemic was survival. The more frequent word combinations were related to the consumption of essential goods: groceries from the `grocery store`, `hand sanitizing` for protection. It is necessary to highlight the `panic buying` behavior that happened at the time. The population was not only commenting about it but also adopting this behavior as all shelves were getting empty, and there was even a shortage on `toilet paper`. Furthermore, the impacts from Covid-19 were also felt on the economy as the `oil prices` suddenly dropped. It is also highlighted through the Trigrams' visualization that has `low oil prices` on the top 10 word combinations.
 
 Finally, this bigram does an excellent job capturing the population's fear of the pandemic and the overall anxiety that has driven the panic buying behavior.


## Trigrams

```{r}
trigram_filtered <- corona_tweets %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co")) %>% 
  filter(!word2 %in% c("https", "19", "t.co")) %>% 
  filter(!word3 %in% c("https", "19", "t.co")) %>% 
  count(word1, word2, word3, sort = TRUE) %>% 
  filter(n > 25)
  
trigram_filtered
```

```{r}
trigram_graph <- trigram_filtered %>% 
  graph_from_data_frame()

trigram_graph
```

```{r}
set.seed(6648)

# plot graph
trigrams_network <- ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = grid::arrow(type = "closed", length = unit(.08, "inches"))) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), 
                 vjust = 1, hjust = 1, size = 3) + 
  theme_void()

trigrams_network
```

### Analysis

 There is not a big difference when comparing the content of the Bigrams and Trigrams; this visualization reinforces what was previously said, the most relevant and frequent trigrams are `grocery store workers`, `stop panic buying`, `low oil prices` which all refer to the pandemic's impact, the panic buying behavior, and how it affects the economy in general. An interesting trigram that appeared on the top 3 most frequent is `loca grocery store`, which refers to people on Twitter creating a movement for the population to support small businesses in their neighborhood.

## Geospatial Bigram Analysis, China

```{r}
china_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "3", "4")) %>% 
  filter(word1 %in% c("china", "chinese", "wuhan") | word2 %in% c("china", "chinese", "wuhan")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
china_bigram_filtered
```

```{r}
china_bigram_graph <- china_bigram_filtered %>% 
  graph_from_data_frame()

china_bigram_graph
```

```{r}
# plot graph
china_bigrams_network <- ggraph(china_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = grid::arrow(type = "closed", length = unit(.08, "inches"))) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), 
                 vjust = 1, hjust = 1, size = 3) + 
  theme_void()

china_bigrams_network
```

### Analysis

Since it was impossible to extract more information from the Bigrams and Trigrams, some words were isolated from commanding the next steps' analysis. For this first example, three names were used, which are: `China`, `Chinese`, and `Wuhan`. This analysis aims to understand the overall sentiment around the Chinese people as there were xenophobic attacks in a subway in New York. 
 
There are some predominant negative bigrams: `China lied`, `China panic`, `stocks China` (referring to stocks drop), and `Chinese virus`. Overall, most of the tweets had this negative connotation and aimed to blame China for what is happening, the most prominent example of it the bigram `Chinese virus`, which is ranked top 1 when it comes to China bigrams.

## Geospatial Bigram Analysis, Italy

```{r}
italy_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(word1 %in% c("italy", "italian", "europe") | word2 %in% c("italy", "italian", "europe")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
italy_bigram_filtered
```

```{r}
italy_bigram_graph <- italy_bigram_filtered %>% 
  graph_from_data_frame()

italy_bigram_graph
```

```{r}
# plot graph
italy_bigrams_network <- ggraph(italy_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = grid::arrow(type = "closed", length = unit(.08, "inches"))) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), 
                 vjust = 1, hjust = 1, size = 3) + 
  theme_void()

italy_bigrams_network
```

### Analysis
 On the other hand, when it comes to analyzing tweets around Italy, which was the most affected country after China, the only tweet pointing it as a Covid-19 spread is the bigram `door italy`. Some other countries, such as Spain and the United Kingdom, are also connected to Italy, making sense. These two were also affected countries at the beginning of the pandemic and had a high number of daily cases and deaths.

## Geospatial Bigram Analysis, California + Florida

```{r}
usa_bigram_filtered <- corona_tweets %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(!word2 %in% c("https", "19", "t.co", "2", "3", "4")) %>% 
  filter(word1 %in% c("usa", "california", " florida", "trump") | word2 %in% c("usa", "california", "florida", "trump")) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2)
  
usa_bigram_filtered
```

```{r}
usa_bigram_graph <- usa_bigram_filtered %>% 
  graph_from_data_frame()

usa_bigram_graph
```

```{r}
# plot graph
usa_bigrams_network <- ggraph( usa_bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n), 
    show.legend = FALSE,  
    end_cap = circle(.07, 'inches'), 
    arrow = grid::arrow(type = "closed", length = unit(.08, "inches"))) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), 
                 vjust = 1, hjust = 1, size = 3) + 
  theme_void()

usa_bigrams_network
```

### Analysis

 Lastly, this geospatial analysis focuses on how the pandemic affected USA states (Florida and California as there was no relevant data on New York) and Trumps' mentions. He was the person in charge to address the population and respond to the pandemic. It is interesting to observe that while Florida's bigrams refer to spatial locations such as __Central Florida__, __Miami Florida__, and __Central Florida__, which are tweets addressing the spread of the virus on the state, California's bigrams show how this state has already started planning a response to the virus: __California launches__ (a plan) and __California governor__. 
 
 Furthermore, when it comes to a response to the pandemic in a country-wide response, there was a big incidence of _Trump_ mentions as he was the president in charge of the response. There were bigrams that stood out which were __trump campaign__, __trump supporter__, __trump toiletpaper__, __trump administration__, __trump drug__ (hydroxychloroquine), and __trump met__. It is important to point out that 2020 was the election year in the United States. The overall feeling shows that the pandemic's response was also aligned with the president's interest in reelection.
 
 
 # Final Remarks

Overall, we found that there is contrast in bigrams when it comes to geospatial regions and coronavirus. While tweets seem to blame China for the existence of the virus, others seem to view California as a state with a plan. Further work could use STM or more sophisticated sentiment analysis techniques in order to build a robust case for regional sentiment differences. 

